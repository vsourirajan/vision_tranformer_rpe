{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "#load CIFAR-10 data\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize((32, 32))\n",
    "])\n",
    "\n",
    "train_data_cifar = datasets.CIFAR10('data_cifar', train=True, download=True, transform=transform)\n",
    "test_data_cifar = datasets.CIFAR10('data_cifar', train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader_cifar = DataLoader(train_data_cifar, batch_size=64, shuffle=True)\n",
    "test_loader_cifar = DataLoader(test_data_cifar, batch_size=64, shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load mnist dataset\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize((32, 32))\n",
    "])\n",
    "\n",
    "train_data_mnist = datasets.MNIST('data_mnist', train=True, download=True, transform=transform)\n",
    "test_data_mnist = datasets.MNIST('data_mnist', train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader_mnist = DataLoader(train_data_mnist, batch_size=64, shuffle=True)\n",
    "test_loader_mnist = DataLoader(test_data_mnist, batch_size=64, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Patch a Single Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 32]) 5\n"
     ]
    }
   ],
   "source": [
    "#patch a single image from the MNIST dataset\n",
    "patch_size = 4\n",
    "image_size = 32\n",
    "\n",
    "image, label = train_data_mnist[0]\n",
    "image = image.squeeze()\n",
    "print(image.shape, label)\n",
    "patches = image.reshape(image_size//patch_size, patch_size, -1, patch_size).swapaxes(1,2).reshape(-1, patch_size, patch_size)\n",
    "\n",
    "# plt.imshow(image)\n",
    "# plt.show()\n",
    "\n",
    "# plt.figure(figsize=(10, 10))\n",
    "# for i in range(patches.shape[0]):\n",
    "#     plt.subplot(8, 8, i+1)\n",
    "#     plt.imshow(patches[i])\n",
    "#     plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RPE Methodologies\n",
    "- General Learnable Function: $f_\\Theta : \\mathbb{R} \\rightarrow \\mathbb{R}$\n",
    "- Monotonically Decreasing Function: $f = e^{-\\alpha x}$\n",
    "- Ratio of two polynomial functions: $f = \\frac{h}{g}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "General Learnable Function: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeneralLearnableFunction(nn.Module):\n",
    "    def __init__(self, embedding_dim):\n",
    "        super(GeneralLearnableFunction, self).__init__()\n",
    "        self.fc = nn.Linear(1, embedding_dim)\n",
    "\n",
    "    def forward(self, distances):\n",
    "        batch_size, num_patches, _ = distances.size()\n",
    "        distances = distances.unsqueeze(-1) \n",
    "        positional_encodings = self.fc(distances)\n",
    "        return positional_encodings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 8, 8, 64])\n",
      "tensor([ 0.5951,  0.7530, -0.6586,  0.2280, -0.6728, -0.2245, -0.7294, -0.1395,\n",
      "         0.5804, -0.4483, -0.9598,  0.5763,  0.6590,  0.3613, -0.0698,  0.5394,\n",
      "        -0.4711,  0.8144,  0.2408,  0.0364, -0.9000, -0.9715,  0.1651, -0.8909,\n",
      "        -0.0119,  0.3047, -0.3610,  0.9064, -0.0058,  0.7433, -0.8007, -0.9259,\n",
      "        -0.6425, -0.0829, -0.1002, -0.1912,  0.9596, -0.8411, -0.0797, -0.8629,\n",
      "         0.2683,  0.7774, -0.6628,  0.8056,  0.0256, -0.3716, -0.2160, -0.8486,\n",
      "         0.0528, -0.9476, -0.4661, -0.2484, -0.7321,  0.9030,  0.1189,  0.8965,\n",
      "         0.9787, -0.4790, -0.4192, -0.0835, -0.4569, -0.6565,  0.1636,  0.8846],\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#testing the general RPE function on a random symmetric distance matrix\n",
    "glf = GeneralLearnableFunction(64)\n",
    "distances = torch.randn(8, 8)\n",
    "distances = distances + distances.transpose(0, 1)\n",
    "distances = distances - torch.diag(distances.diagonal())\n",
    "distances = distances.unsqueeze(0)\n",
    "positional_encodings = glf(distances)\n",
    "print(positional_encodings.shape)\n",
    "print(positional_encodings[0, 0, 0]) #should be embedding of dimension 64 with random values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Monotonically-Decreasing Function: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MonotonicFunction(nn.Module):\n",
    "    def __init__(self, embedding_dim):\n",
    "        super(MonotonicFunction, self).__init__()\n",
    "        self.alpha = nn.Parameter(torch.randn(1))\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "    def forward(self, distances):\n",
    "        batch_size, num_patches, _ = distances.size()\n",
    "        decay_factor = torch.exp(-self.alpha * distances)\n",
    "        decay_factor_normalized = decay_factor / torch.sum(decay_factor, dim=-1, keepdim=True)\n",
    "        positional_encodings = decay_factor_normalized.unsqueeze(-1).expand(batch_size, num_patches, num_patches, self.embedding_dim)\n",
    "        \n",
    "        return positional_encodings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ratio of Two Polynomials: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Absolute Positional Encoding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#regular positonal encoding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer Encoder Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderLayer():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationHead(nn.Module):\n",
    "    def __init(self, embedding_dim, num_classes):\n",
    "        super(ClassificationHead, self).__init__()\n",
    "        self.fc = nn.Linear(embedding_dim, num_classes)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
