{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "#load CIFAR-10 data\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize((32, 32))\n",
    "])\n",
    "\n",
    "train_data_cifar = datasets.CIFAR10('data_cifar', train=True, download=True, transform=transform)\n",
    "test_data_cifar = datasets.CIFAR10('data_cifar', train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader_cifar = DataLoader(train_data_cifar, batch_size=64, shuffle=True)\n",
    "test_loader_cifar = DataLoader(test_data_cifar, batch_size=64, shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load mnist dataset\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize((32, 32))\n",
    "])\n",
    "\n",
    "train_data_mnist = datasets.MNIST('data_mnist', train=True, download=True, transform=transform)\n",
    "test_data_mnist = datasets.MNIST('data_mnist', train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader_mnist = DataLoader(train_data_mnist, batch_size=64, shuffle=True)\n",
    "test_loader_mnist = DataLoader(test_data_mnist, batch_size=64, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Patch a Single Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 32]) 5\n"
     ]
    }
   ],
   "source": [
    "#patch a single image from the MNIST dataset\n",
    "patch_size = 4\n",
    "image_size = 32\n",
    "\n",
    "image, label = train_data_mnist[0]\n",
    "image = image.squeeze()\n",
    "print(image.shape, label)\n",
    "patches = image.reshape(image_size//patch_size, patch_size, -1, patch_size).swapaxes(1,2).reshape(-1, patch_size, patch_size)\n",
    "\n",
    "# plt.imshow(image)\n",
    "# plt.show()\n",
    "\n",
    "# plt.figure(figsize=(10, 10))\n",
    "# for i in range(patches.shape[0]):\n",
    "#     plt.subplot(8, 8, i+1)\n",
    "#     plt.imshow(patches[i])\n",
    "#     plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Patching a Batch of Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, image_size, patch_size, in_channels, embed_dim):\n",
    "        super(PatchEmbedding, self).__init__()\n",
    "        self.image_size = image_size\n",
    "        self.patch_size = patch_size\n",
    "        self.projection = nn.Conv2d(in_channels, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.projection(x)  # (batch_size, embed_dim, num_patches_w, num_patches_h)\n",
    "        x = x.permute(0, 2, 3, 1)  # (batch_size, num_patches_w, num_patches_h, embed_dim)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1, 32, 32]) tensor([3, 2, 3, 1, 3, 8, 9, 1, 9, 0, 8, 5, 2, 6, 2, 7, 3, 2, 8, 6, 2, 2, 8, 0,\n",
      "        7, 8, 3, 3, 2, 7, 2, 5, 3, 3, 4, 3, 5, 6, 7, 1, 9, 2, 0, 3, 5, 3, 3, 3,\n",
      "        5, 0, 0, 4, 0, 7, 1, 7, 3, 9, 4, 8, 3, 8, 9, 3])\n",
      "torch.Size([64, 8, 8, 64])\n"
     ]
    }
   ],
   "source": [
    "#testing the PatchEmbedding class on a batch of the MNIST dataset\n",
    "image, label = next(iter(train_loader_mnist))\n",
    "print(image.shape, label)\n",
    "\n",
    "patch_size = 4\n",
    "image_size = 32\n",
    "in_channels = 1\n",
    "embed_dim = 64\n",
    "\n",
    "patch_embed = PatchEmbedding(image_size, patch_size, in_channels, embed_dim)\n",
    "x = patch_embed(image)\n",
    "print(x.shape)\n",
    "#looks right...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Absolute Positional Encoding (RPE-free Baseline)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing the regular positional encoding on "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RPE Methodologies\n",
    "- General Learnable Function: $f_\\Theta : \\mathbb{R} \\rightarrow \\mathbb{R}$\n",
    "- Monotonically Decreasing Function: $f = e^{-\\alpha x}$\n",
    "- Ratio of two polynomial functions: $f = \\frac{h}{g}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "General Learnable Function: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeneralLearnableFunction(nn.Module):\n",
    "    def __init__(self, embedding_dim):\n",
    "        super(GeneralLearnableFunction, self).__init__()\n",
    "        self.fc = nn.Linear(1, embedding_dim)\n",
    "\n",
    "    def forward(self, distances):\n",
    "        batch_size, num_patches, _ = distances.size()\n",
    "        distances = distances.unsqueeze(-1) \n",
    "        positional_encodings = self.fc(distances)\n",
    "        return positional_encodings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 8, 8, 64])\n",
      "tensor([ 0.5504, -0.0272,  0.8280, -0.0335, -0.5601, -0.1169,  0.9159,  0.4995,\n",
      "        -0.0534,  0.4756, -0.8882,  0.1858,  0.2654,  0.0998,  0.3770,  0.0293,\n",
      "         0.7467, -0.8272,  0.7443, -0.1012,  0.2873, -0.4944, -0.9595, -0.4560,\n",
      "         0.9493, -0.4706,  0.9414,  0.9434, -0.7811,  0.9605, -0.8934,  0.0583,\n",
      "         0.5981, -0.4510, -0.8587,  0.2166,  0.9725,  0.0690, -0.7834, -0.1792,\n",
      "        -0.4014, -0.0596, -0.1858,  0.2153,  0.1585, -0.9607, -0.0823,  0.9454,\n",
      "        -0.3373, -0.7897,  0.9248, -0.4429,  0.1606,  0.2784, -0.3660,  0.4903,\n",
      "        -0.6577,  0.3554, -0.0659,  0.9351,  0.7247,  0.3084,  0.8950,  0.2198],\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#testing the general RPE function on a random symmetric distance matrix\n",
    "glf = GeneralLearnableFunction(64)\n",
    "distances = torch.randn(8, 8)\n",
    "distances = distances + distances.transpose(0, 1)\n",
    "distances = distances - torch.diag(distances.diagonal())\n",
    "distances = distances.unsqueeze(0)\n",
    "positional_encodings = glf(distances)\n",
    "print(positional_encodings.shape)\n",
    "print(positional_encodings[0, 0, 0]) #should be embedding of dimension 64 with random values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.3224e-01, -1.7628e-03, -1.6146e-01,  3.1187e-02,  1.6961e-01,\n",
      "         2.0496e-01, -6.3421e-02, -1.2596e-01, -6.0501e-02,  1.7331e-01,\n",
      "        -1.1854e-01, -2.4561e-02, -1.2756e-02, -5.9731e-02, -1.0772e-02,\n",
      "        -8.2388e-02, -8.3777e-02, -1.5694e-01,  4.7398e-02,  1.2669e-02,\n",
      "         2.1559e-01, -1.3391e-01,  5.3702e-02,  1.1682e-01,  2.1324e-01,\n",
      "        -4.7466e-02, -9.0230e-02,  1.9399e-01, -1.5392e-02,  1.2149e-01,\n",
      "         1.4484e-01,  2.0822e-01,  1.3077e-01, -9.0394e-02, -1.4869e-01,\n",
      "         1.9546e-02, -1.5460e-01,  2.2889e-01,  1.0046e-04, -6.6848e-02,\n",
      "        -2.4300e-01, -1.4527e-01, -2.3898e-01, -1.7482e-01, -1.9071e-01,\n",
      "         2.2830e-01, -1.4923e-01,  2.3880e-01,  1.3067e-01,  1.3353e-01,\n",
      "        -1.4982e-01, -1.6649e-01,  1.0929e-01,  1.1080e-01, -7.0484e-02,\n",
      "        -4.4598e-02, -2.4193e-01,  1.7137e-01, -5.9003e-02, -1.0231e-01,\n",
      "        -1.6886e-01, -4.9098e-02,  1.0148e-01, -3.0133e-02],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "torch.Size([64, 8, 8, 64])\n",
      "tensor([ 0.4181, -0.0290,  0.6665, -0.0023, -0.3905,  0.0880,  0.8525,  0.3735,\n",
      "        -0.1139,  0.6490, -1.0068,  0.1613,  0.2527,  0.0401,  0.3663, -0.0531,\n",
      "         0.6629, -0.9841,  0.7917, -0.0885,  0.5029, -0.6283, -0.9058, -0.3392,\n",
      "         1.1625, -0.5181,  0.8512,  1.1374, -0.7965,  1.0820, -0.7485,  0.2665,\n",
      "         0.7288, -0.5414, -1.0074,  0.2361,  0.8179,  0.2979, -0.7833, -0.2460,\n",
      "        -0.6444, -0.2049, -0.4248,  0.0405, -0.0322, -0.7324, -0.2316,  1.1842,\n",
      "        -0.2066, -0.6562,  0.7750, -0.6094,  0.2699,  0.3892, -0.4365,  0.4457,\n",
      "        -0.8996,  0.5268, -0.1249,  0.8328,  0.5558,  0.2593,  0.9965,  0.1897],\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#test adding the positional encodings to a batch of patch embeddings\n",
    "\n",
    "print(x[0, 0, 0])\n",
    "encoded_patches = x + positional_encodings\n",
    "print(encoded_patches.shape)\n",
    "print(encoded_patches[0, 0, 0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.0657, -0.3909,  0.3240,  0.3323, -0.1932, -0.1612,  0.5429, -0.4009,\n",
      "        -0.9807, -0.2675,  0.2946, -0.9324, -0.2248,  0.0372, -0.7210, -0.8905,\n",
      "        -0.8576, -0.6487,  0.4725,  0.5766, -0.2445, -0.4656, -0.5605, -0.3511,\n",
      "         0.2134,  0.4169, -0.5610,  1.0331,  0.2065,  0.2802, -0.2306,  0.1593,\n",
      "         0.2018, -0.6545,  1.1062, -0.5613, -0.1923,  0.4613,  0.6055,  0.8612,\n",
      "        -0.0611, -0.6883,  0.1176, -0.0898,  0.2557,  0.9772,  1.0901,  0.5571,\n",
      "         0.3595,  0.6526, -0.7675, -1.1276,  0.5628, -0.6863,  0.3676,  0.3017,\n",
      "        -0.7870,  0.0079, -0.5558,  0.9946,  0.0598,  0.3599,  0.6744,  0.1113],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "tensor([ 4.8467e-01, -4.1807e-01,  1.1520e+00,  2.9883e-01, -7.5329e-01,\n",
      "        -2.7814e-01,  1.4588e+00,  9.8546e-02, -1.0341e+00,  2.0816e-01,\n",
      "        -5.9358e-01, -7.4662e-01,  4.0611e-02,  1.3697e-01, -3.4401e-01,\n",
      "        -8.6118e-01, -1.1085e-01, -1.4758e+00,  1.2168e+00,  4.7540e-01,\n",
      "         4.2842e-02, -9.5998e-01, -1.5201e+00, -8.0716e-01,  1.1626e+00,\n",
      "        -5.3673e-02,  3.8046e-01,  1.9765e+00, -5.7468e-01,  1.2406e+00,\n",
      "        -1.1240e+00,  2.1762e-01,  7.9982e-01, -1.1055e+00,  2.4749e-01,\n",
      "        -3.4473e-01,  7.8016e-01,  5.3027e-01, -1.7791e-01,  6.8199e-01,\n",
      "        -4.6246e-01, -7.4791e-01, -6.8211e-02,  1.2551e-01,  4.1423e-01,\n",
      "         1.6439e-02,  1.0078e+00,  1.5025e+00,  2.2236e-02, -1.3717e-01,\n",
      "         1.5725e-01, -1.5705e+00,  7.2341e-01, -4.0790e-01,  1.6190e-03,\n",
      "         7.9190e-01, -1.4448e+00,  3.6335e-01, -6.2174e-01,  1.9297e+00,\n",
      "         7.8451e-01,  6.6829e-01,  1.5695e+00,  3.3110e-01],\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(x[5, 5, 5])\n",
    "print(encoded_patches[5, 5, 5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this works!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Monotonically-Decreasing Function: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MonotonicFunction(nn.Module):\n",
    "    def __init__(self, embedding_dim):\n",
    "        super(MonotonicFunction, self).__init__()\n",
    "        self.alpha = nn.Parameter(torch.randn(1))\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "    def forward(self, distances):\n",
    "        batch_size, num_patches, _ = distances.size()\n",
    "        decay_factor = torch.exp(-self.alpha * distances)\n",
    "        decay_factor_normalized = decay_factor / torch.sum(decay_factor, dim=-1, keepdim=True)\n",
    "        positional_encodings = decay_factor_normalized.unsqueeze(-1).expand(batch_size, num_patches, num_patches, self.embedding_dim)\n",
    "        \n",
    "        return positional_encodings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ratio of Two Polynomials: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationHead(nn.Module):\n",
    "    def __init(self, embedding_dim, num_classes):\n",
    "        super(ClassificationHead, self).__init__()\n",
    "        self.fc = nn.Linear(embedding_dim, num_classes)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vision Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViT(nn.Module):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
